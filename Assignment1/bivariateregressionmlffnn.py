# -*- coding: utf-8 -*-
"""BivariateRegressionMLFFNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LyHBtntMrM2sO9K8UYqIWxSqbJ7gQtJG
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random
import math
# %matplotlib inline

plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (10, 10)

from google.colab import drive
drive.mount('/content/drive')

File="/content/drive/My Drive/Deeplearning/Group31_Assignment1/"

def readData(file):
    data=pd.read_csv(file+".csv",",",header=None)
    data=np.array(data.values)
    return data

def train_test_split(arr, test):
    y=np.full(len(arr),1)
    
    train=     np.array([y[:-int(len(arr)*(test))],arr[:-int(len(arr)*(test)),0],arr[:-int(len(arr)*(test)),1],arr[:-int(len(arr)*(test)),2]])
    test=      np.array([y[-int(len(arr)*test): ],arr[-int(len(arr)*test): ,0],arr[-int(len(arr)*test): ,1],arr[-int(len(arr)*test): ,2]])
    
    return train.T,  test.T

def plot_data(red,green, blue,title):
    plt.scatter(red[0],red[1],color='red')
    plt.scatter(green[0],green[1],color='green')
    plt.scatter(blue[0],blue[1],color='blue')
    plt.title(title)
    plt.savefig(title)
    plt.show()

Folder=[File+"Group31/Regression/BivariateData/31"]

for k in Folder:
        
    folder=k
    data=  readData(k)
    
    train_data, Test=  train_test_split(data,0.20)



# Decision boundary
def generate_values_boundary(data,weights,archicture,ttl):
    min_x=min(data[:,1])
    max_x=max(data[:,1])
    min_y=min(data[:,2])
    max_y=max(data[:,2])
    
    x = np.linspace(min_x-0.1, max_x+0.1, 100)
    y = np.linspace(min_y-0.1, max_y+0.1, 100)
    xx, yy = np.meshgrid(x, y)
    
    D=[]
    for i in x:
        for j in y:
            D.append([1,i,j])

    
    network_outputs=[]
    for i in range(len(D)):

        input_data=np.array(D[i])
#         network_outputs.append(input_data) 
#         print(input_data)
        #Calculation of the weighted sum at the every layer of the neural network
        for h in range(len(archicture)-1):

            if h== len(archicture)-2:
                output_vector=np.dot(weights[h],input_data)
#                 print(output_vector)
            else:
                output_vector=perceptron(weights[h],input_data,sigmoid)
#                 print(output_vector)
            input_data=np.concatenate(([1],output_vector))
            
        network_outputs.append(output_vector[0])


    z= np.array(network_outputs)
#     print(data[:,2])  
#     print(z)
    A=np.array(D)
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.scatter3D(A[:,1],A[:,2],z,zorder=15,color='#99C2DE',alpha=0.3)
    ax.scatter3D(data[:,1],data[:,2],data[:,3],marker='D',color='#EAA358',edgecolors='black')
    
    
    
    print(ttl)
    plt.xlabel("x1")  # add X-axis label 
    plt.ylabel("x2") 
#     plt.savefig(ttl+'.png',bbox_inches = 'tight')
    plt.show()

# Decision boundary
def generate_values_boundary2(data,weights,archicture,ttl):
   

    
    network_outputs=[]
    for i in range(len(data)):

        input_data=np.array(data[i][:3])
#         network_outputs.append(input_data) 
#         print(input_data)
        #Calculation of the weighted sum at the every layer of the neural network
        for h in range(len(archicture)-1):

            if h== len(archicture)-2:
                output_vector=np.dot(weights[h],input_data)
#                 print(output_vector)
            else:
                output_vector=perceptron(weights[h],input_data,sigmoid)
#                 print(output_vector)
            input_data=np.concatenate(([1],output_vector))
            
        network_outputs.append(output_vector[0])


    z= np.array(network_outputs)
#     print(data[:,3])  
#     print(z)
    plt.scatter(data[:,3],z,marker='D',color='#EAA358',edgecolors='black')
    
    
    
    print(ttl)
    plt.xlabel("Target Output")  # add X-axis label 
    plt.ylabel("Model Output") 
#     plt.savefig(ttl+'.png',bbox_inches = 'tight')
    plt.show()



fig = plt.figure()
ax = plt.axes(projection='3d')
ax.scatter3D(data[:,0],data[:,1],data[:,2],marker='D',color='#EAA358',edgecolors='black')
# xx, yy = np.meshgrid(range(10), range(10))
# z = (9 - xx - yy) / 2 
# ax.plot_surface(xx, yy, z, alpha=0.5)
plt.show()

def logistic(input):
    x=np.array(input)
    return 1/(1+np.exp(-x))

def perceptron(weights,input,sigmoid):
    
    predict=np.dot(weights,input)
    
    if(sigmoid):
        predict=np.tanh(predict)
        
    elif sigmoid==2:
        return predict
        
    else:
        predict=logistic(predict)
        
    return predict

def reg_derivative(x,sigmoid):
    

        
    return 1

def activation_derivative(x,sigmoid):
    
    if sigmoid:
        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
        dt=1-t**2
    
    else:
        dt= x * (1.0 - x)
        
    return dt

def calculate_delta(a,b):
    
    c=np.zeros(a.shape)
    for i in range(a.shape[0]):
        for j in range(a.shape[1]):

            if j!=0:
                c[i][j]=a[i][j]*b[i]
    
    return np.sum(c,axis=0)[1:]

def network(data,archicture,epoch,sigmoid,learning_rate):
    
    weights=[]

    for h in range(len(archicture)-1):
        weight=np.array([[random.seed(i+j) or random.random() for i in range(archicture[h]+1)] for j in range(archicture[h+1])])
        weights.append(weight)
        
    Error_epoch=list()
    
    
    for j in range(epoch):
        

        output=list()
        expected=list()
        np.random.shuffle(data)
        
        for i in range(len(data)):
            
            input_data=np.copy(data[i][:3])
        
            network_outputs=[]
            network_outputs.append(input_data) 
            
            #Calculation of the weighted sum at the every layer of the neural network
            for h in range(len(archicture)-1):
                
                if h== len(archicture)-2:
                    output_vector=np.dot(weights[h],input_data)
                else:
                    output_vector=perceptron(weights[h],input_data,sigmoid)
                    
                input_data=np.concatenate(([1],output_vector))
                network_outputs.append(input_data) 
#             print('output',input_data)
           
            #Send the data for backpropogation 
            target=np.copy(data[i][3])  
            weights=back_propagate(archicture,target,network_outputs,weights, learning_rate,sigmoid) 
            output.append(network_outputs[-1][1:])
            expected.append(list(data[i][3:]))
            
            
        #Calculate the MSE for the every epoch  
        error_output=np.array(expected,dtype=np.float128)-np.array(output,dtype=np.float128)
        cost=(np.sum(error_output*error_output,axis=0)/len(error_output))
        Etotal=np.sum(cost)
        Error_epoch.append(Etotal)
        
        #print the error of every output neuron in every 100 epochs
        if (j+1)%100==0 or j==0:
            print('Epoch',j+1,': ',cost)

        
    
    return Error_epoch,weights

def back_propagate(archicture,target,network_outputs,weights, learning_rate,sigmoid):
    
    Delta=[]
    length=len(archicture)
    
    #Calculate the delta for the final output layer
    Do=-2*((target-network_outputs[-1:][0][1:]))*reg_derivative(1,sigmoid)
    Delta.append(Do)

    
    #Claculate the delta for all internal layers
    for h in reversed (range(1,length-1)):
        new_Delta=calculate_delta(weights[h],Delta[-1])
        new_Delta=new_Delta*activation_derivative(network_outputs[h-length][1:],sigmoid)
        Delta.append(new_Delta)
    Delta.reverse()
    
    #Calculate the new updated value for the weights   
    
    k=-1
    for W in weights:
        k=k+1
#         print('w ',W)
        for i in range(W.shape[0]):
            for j in range(W.shape[1]):
                W[i][j]=W[i][j]-learning_rate*network_outputs[k][j]*Delta[k][i]

        
    return weights

def test(data,weights,archicture,sigmoid):
    

        output=list()
        neu_output=list()
        expected=list()
        np.random.shuffle(data)
        
        for i in range(len(data)):
            
            input_data=np.copy(data[i][:3])
        
            network_outputs=[]
            network_outputs.append(input_data) 
            
            #Calculation of the weighted sum at the every layer of the neural network
            for h in range(len(archicture)-1):
                
                if h== len(archicture)-2:
                    output_vector=np.dot(weights[h],input_data)
                else:
                    output_vector=perceptron(weights[h],input_data,sigmoid)
                    neu_output.append(output_vector)
                    
                input_data=np.concatenate(([1],output_vector))
                network_outputs.append(input_data) 
#             print('output',input_data)
           
            #Send the data for backpropogation 
            target=np.copy(data[i][3])  
            output.append(network_outputs[-1][1:])
            expected.append(list(data[i][3:]))
            
            
        #Calculate the MSE for the every epoch  
        error_output=np.array(expected)-np.array(output)
        cost=(np.sum(error_output*error_output,axis=0)/len(error_output))
        Etotal=np.sum(cost)
        
       

        
    
        return Etotal,neu_output

def classifier(data,archicture,epoch,sigmoid,learning_rate,file):
    
    error_output,weights=network(np.copy(data),archicture,epoch,sigmoid,learning_rate)
    
    plt.plot(error_output)
    plt.xlabel("Epochs")  # add X-axis label 
    plt.ylabel("Error")  # add Y-axis label 
    plt.title("Error Plot")  # add title 
    plt.savefig(file+'Figure/Bi_Error.png',bbox_inches = 'tight')


    
    return weights

def cross_validation(k,Arr):



    length1=int(len(Arr)*0.25)

    
    M1,M2,M3,M4=Arr[:length1].copy(),Arr[length1:2*length1],Arr[2*length1:3*length1],Arr[3*length1:4*length1]
   
    if k==1:
        M=np.concatenate((M2,M3,M4))
        V=M1
    if k==2:
        M=np.concatenate((M1,M3,M4))
        V=M2
    if k==3:
        M=np.concatenate((M1,M2,M4))
        V=M3
    if k==4:
        M=np.concatenate((M1,M2,M3))
        V=M4


    
    
    return M, V



#archicture[no_of_input,no_of_neuron_h1,no_of_neuron_h2,no_of_output]
#archicture[no_of_input,no_of_neuron_h1,no_of_output]
#sigmoid=0 means logistic and sigmoid=1 means tanh

archicture=[2,4,1]
sigmoid=0
learning_rate=0.1
epoch=10
k=4 # k=1, 2, 3, 4 for the cross validation, where 1 is Fold-1
plt.rcParams['figure.figsize'] = (10, 10)


Train,Validation=cross_validation(k,train_data)
Weights=classifier(Train,archicture,epoch,sigmoid,learning_rate,File)



generate_values_boundary(Train,Weights,archicture,'Regression')





A1,output1=test(Train,Weights,archicture,sigmoid)

A2,output2=test(Validation,Weights,archicture,sigmoid)

A3,output3=test(Test,Weights,archicture,sigmoid)

print('Training Error: ',A1)

print('Validation Error: ',A2)

print('Test Error: ',A3)

a=[0.009,0.007,0.007,0.007]
b=[0.009,0.007,0.007,0.007]
c=[0.009,0.008,0.007,0.007]
xticks = ['Model 1','Model 2','Model 3','Model 4']

plt.plot(xticks,a,label='Train Error')
plt.plot(xticks,b,label='Validation Error')
plt.plot(xticks,c,label='Test Error')
plt.ylabel("MSE")  # add X-axis label 
plt.xlabel("Model Complexity") 
plt.legend()

def plot(data,output):
    
    plt.rcParams['figure.figsize'] = (20, 30)
    # plt.rcParams['axes.facecolor']='w'
    fig = plt.figure()
    ax1 = fig.add_subplot(421, projection='3d')
    ax2 = fig.add_subplot(422, projection='3d')
    ax3 = fig.add_subplot(423, projection='3d')
    ax4 = fig.add_subplot(424, projection='3d')
#     ax5 = fig.add_subplot(425, projection='3d')
#     ax6 = fig.add_subplot(426, projection='3d')
#     ax7 = fig.add_subplot(427, projection='3d')
#     ax8 = fig.add_subplot(428, projection='3d')

    axs=[ax1,ax2,ax3,ax4]

    for j in range(len(axs)):
        axs[j].set_xlabel('X Label')
        axs[j].set_ylabel('Y Label')
        axs[j].set_zlabel('Z Label') 

    for j in range(len(axs)):

        for i in range (len(data)):

            # Data for three-dimensional scattered points
            r=j+1
            l=r-1

            zdata = output[i][l:r]
            xdata = data[i][1:2]
            ydata = data[i][2:3]
            axs[j].scatter(xdata, ydata, zdata, c=zdata, cmap='autumn',edgecolors='black');


    plt.savefig('Figure/Neuron_output.png',bbox_inches = 'tight')

plot(test_data,output3)

def neuron_output(data,output,neuron,hidden,file,ttl):

  plt.rcParams['figure.figsize'] = (20, 30)
  # plt.rcParams['axes.facecolor']='w'
  fig = plt.figure()
  axs=[]
  a=int((neuron/2)+0.5)

  for i in range(neuron):

    b=100*a+20+i+1

    ax = fig.add_subplot(b, projection='3d')
    axs.append(ax)


  # axs=[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8]

  for j in range(len(axs)):
      axs[j].set_xlabel('X Input')
      axs[j].set_ylabel('Y Input')
      axs[j].set_zlabel('Z Output') 

  for j in range(len(axs)):

      for i in range (len(Train)):

          # Data for three-dimensional scattered points
          r=j+1
          l=r-1
          
          zdata = output[i+hidden][l:r] #hidden=0 for the hidden layer 1, and hidden=1 for the output layer
          xdata = data[i][1:2]
          ydata = data[i][2:3]
          axs[j].scatter(xdata, ydata, zdata, c=zdata, cmap='autumn',edgecolors='black');


  plt.savefig(file+'Figure/'+ttl+'.png',bbox_inches = 'tight')

neuron_output(Test,output3,4,0,File,'Bivariate_Hidden_Neuron_Train')

