# -*- coding: utf-8 -*-
"""NonLinearlySeperableDataWithPerceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pw3mjpEzDwPusluIFiEpbQhMrNKoTh_-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
# %matplotlib inline

plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (10, 10)

from google.colab import drive
drive.mount('/content/drive')

File="/content/drive/My Drive/Deeplearning/Group31_Assignment1/"

def readData(Folder,file):
    data=pd.read_csv(Folder+"/"+str(file)+".txt"," ",header=None)
    data=np.array(data.values)
    return data

def train_test_split(arr, validation, test):
    y=np.full(len(arr),1)
    
    train=     np.array([y[:-int(len(arr)*(validation+test))],arr[:-int(len(arr)*(validation+test)),0],arr[:-int(len(arr)*(validation+test)),1]])
    validation=np.array([y[-int(len(arr)*(validation+test)):-int(len(arr)*(validation))],arr[-int(len(arr)*(validation+test)):-int(len(arr)*(validation)),0],arr[-int(len(arr)*(validation+test)):-int(len(arr)*(validation)),1]])
    test=      np.array([y[-int(len(arr)*test): ],arr[-int(len(arr)*test): ,0],arr[-int(len(arr)*test): ,1]])
    
    return train.T, validation.T, test.T

def plot_data(red,green, blue,title):
    plt.scatter(red[0],red[1],color='red')
    plt.scatter(green[0],green[1],color='green')
    plt.scatter(blue[0],blue[1],color='blue')
    plt.title(title)
    plt.savefig(title)
    plt.show()

Folder=[File+"Group31/Classification/NLS_Group31"]

for k in Folder:
        
    folder=k
    red=  readData(k,'Class1')
    green=readData(k,'Class2')
    blue= readData(k,'Class3')
    
    red,   valid_red,   test_red=  train_test_split(red,  0.20,0.20)
    green, valid_green, test_green=train_test_split(green,0.20,0.20)
    blue,  valid_blue,  test_blue= train_test_split(blue, 0.20,0.20)

# Decision boundary
def generate_values_boundary(data,data_size,weights1,weights2,weights3,ttl):
    min_x=min(data[:,1])
    max_x=max(data[:,1])
    min_y=min(data[:,2])
    max_y=max(data[:,2])
    W11=weights1[1]
    W12=weights1[2]
    b1= weights1[0]
    W21=weights2[1]
    W22=weights2[2]
    b2= weights2[0]
    W31=weights3[1]
    W32=weights3[2]
    b3= weights3[0]
    x_mesh = np.linspace(min_x-5, max_x+5, 500)
    y_mesh = np.linspace(min_y-5 ,max_y+5, 500)
    y_red   = -(W11/W12)*x_mesh-b1/W12
    y_green = -(W21/W22)*x_mesh-b2/W22
    y_blue  = -(W31/W32)*x_mesh-b3/W32
#     plt.plot(x_mesh,y_red,color='red',label='p1')
#     plt.plot(x_mesh,y_green,color='green',label='p2')
#     plt.plot(x_mesh,y_blue,color='blue',label='p3')
#     plt.legend()
    red_bound = [];
    green_bound = [];
    blue_bound = [];
    for i in range(x_mesh.size):
            for j in range(y_mesh.size):
                x = y_mesh[j]
                y_red   = -(W11/W12)*x_mesh[i]-b1/W12
                y_green = -(W21/W22)*x_mesh[i]-b2/W22
                y_blue  = -(W31/W32)*x_mesh[i]-b3/W32
                if (x<y_red and x>y_green):
                    red_bound.append([x_mesh[i], y_mesh[j]])
                elif (x>y_red and x>y_blue):
                    green_bound.append([x_mesh[i], y_mesh[j]])
                elif (x<y_blue):
                    blue_bound.append([x_mesh[i], y_mesh[j]])
                    
    boundR = np.asarray(red_bound)
    boundG = np.asarray(green_bound)
    boundB = np.asarray(blue_bound)
    if(boundR.size==0):
        boundR = np.asarray([[0, 0],[0, 0]])

    plt.scatter(boundR[:,0], boundR[:,1], color='#99C2DE')
    plt.scatter(boundG[:,0], boundG[:,1], color='#FFC593')
    plt.scatter(boundB[:,0], boundB[:,1], color='#75993a')
    plt.scatter(data[:data_size[0],1],data[:data_size[0],2],marker='D',color='#EAA358',edgecolors='black')
    plt.scatter(data[data_size[0]:data_size[0]+data_size[1],1],data[data_size[0]:data_size[0]+data_size[1],2],marker='X',color='#65993a',edgecolors='black')
    plt.scatter(data[data_size[0]+data_size[1]:,1],data[data_size[0]+data_size[1]:,2],marker='o',color='#4486B7',edgecolors='black')
    
    print(ttl+' Decision Region')
    plt.xlabel("x1")  # add X-axis label 
    plt.ylabel("x2") 
    plt.savefig(ttl+'.png',bbox_inches = 'tight')
    plt.show()

plt.scatter(red[:,1],red[:,2],marker='D',color='#EAA358',edgecolors='black')
plt.scatter(green[:,1],green[:,2],marker='X',color='#65993a',edgecolors='black')
plt.scatter(blue[:,1],blue[:,2],marker='o',color='#4486B7',edgecolors='black')
plt.xlabel("x1")  # add X-axis label 
plt.ylabel("x2") 
plt.savefig('data.png',bbox_inches = 'tight')



def logistic(input):
    x=np.array(input,dtype=np.float128)
    return 1/(1+np.exp(-x))

def perceptron(weights,input,log):
    
    predict=np.dot(weights,input)
    if(log):
        predict=logistic(predict)
        t=0.5
            
    else:
        predict=math.tanh(predict)
        t=0

    if predict>t:
        predict=1
    else:
        predict=0
                
    return predict

def network(red, green, blue,epoch,log):
    
    data1=np.concatenate((red,green))
    data2=np.concatenate((red,blue))
    data3=np.concatenate((blue,green))
    y1=np.concatenate((np.full(len(red), 0),  np.full(len(green),1)))
    y2=np.concatenate((np.full(len(red), 0),  np.full(len(blue), 1)))
    y3=np.concatenate((np.full(len(blue),0),  np.full(len(green),1)))
    w1=0.1
    w2=0.1
    bias=0.1
    cls1=0
    l_rate=0.001
    epoch_error=list()
    error1=0
    error2=0
    error3=0
    error_vector1=[0,0,0,0]
    error_vector2=[0,0,0,0]
    error_vector3=[0,0,0,0]
    weights1=np.array([bias,w1,w2])
    weights2=np.array([bias,w1,w2])
    weights3=np.array([bias,w1,w2])

    for j in range(epoch):
        
        

        error_vector1[3]=0
        error_vector2[3]=0
        error_vector3[3]=0
        for i in range(len(data1)):

            expected1=y1[i]
            expected2=y2[i]
            expected3=y3[i]
            error1=0
            error2=0
            error3=0
            
            predict1=perceptron(weights1,data1[i],log)
            predict2=perceptron(weights2,data2[i],log)
            predict3=perceptron(weights3,data3[i],log)
            
            if  predict1==0:
                error1=expected1-predict1
      
            else:
                error1=expected1-predict1
                
            if predict2==0:
                error2=expected2-predict2
                    
            else:
                error2=expected2-predict2
                
            if predict3==0:
                error3=expected3-predict3
  
            else:
                error3=expected3-predict3

            error_vector1=[error_vector1[0]+error1*l_rate,error_vector1[1]+error1*l_rate*data1[i][1],error_vector1[2]+error1*l_rate*data1[i][2],error_vector1[3]+abs(error1)]
            error_vector2=[error_vector2[0]+error2*l_rate,error_vector2[1]+error2*l_rate*data2[i][1],error_vector2[2]+error2*l_rate*data2[i][2],error_vector2[3]+abs(error2)]
            error_vector3=[error_vector3[0]+error3*l_rate,error_vector3[1]+error3*l_rate*data3[i][1],error_vector3[2]+error3*l_rate*data3[i][2],error_vector3[3]+abs(error3)]
        weights1=[weights1[0]+error_vector1[0],weights1[1]+error_vector1[1],weights1[2]+error_vector1[2]]
        weights2=[weights2[0]+error_vector2[0],weights2[1]+error_vector2[1],weights2[2]+error_vector2[2]]
        weights3=[weights3[0]+error_vector3[0],weights3[1]+error_vector3[1],weights3[2]+error_vector3[2]]
        epoch_error.append((error_vector1[3]+error_vector2[3]+error_vector3[3])/len(data1))
        
    return weights1,weights2,weights3,epoch_error

def test(test_data,test_y,weights1,weights2,weights3,log=1):   
    cls=3  
  
    # Confusion Matrix
    confusion = np.zeros((3,3))

    for i in range(test_data.shape[0]):

        z=test_y[i]

        # Weighted sum of inputs
        predict1=perceptron(weights1,test_data[i],log)
        predict2=perceptron(weights2,test_data[i],log)
        predict3=perceptron(weights3,test_data[i],log)

        if  predict1==0 and predict2==0 and predict3==0:#1
            cls=1
        if  predict1==0 and predict2==0 and predict3==1:#1
            cls=0
        if  predict1==0 and predict2==1 and predict3==0:#1
            cls=2
        if  predict1==0 and predict2==1 and predict3==1:#1
            cls=1
        if  predict1==1 and predict2==0 and predict3==0:#1
            cls=1
        if  predict1==1 and predict2==0 and predict3==1:#1
            cls=1
        if  predict1==1 and predict2==1 and predict3==0:#1
            cls=2
        if  predict1==1 and predict2==1 and predict3==1:#1
            cls=2
#         if  predict1==0 and predict2==1:
#             cls=0
#         if  predict1==1 and predict2==0:
#             cls=2
#         if  predict1==1 and predict3==1:
#             cls=2
            
#         if  predict1==1 and predict3==1:#2
#             cls=0
#         if  predict1==0 and predict3==0:#2
#             cls=2
#         if  predict1==1 and predict3==0:#2
#             cls=0
#         if  predict1==0 and predict3==1:#2
#             cls=0
            
#         if  predict3==0 and predict2==0:#3
#             cls=2
#         if  predict3==0 and predict2==1:#3
#             cls=2
#         if  predict3==1 and predict2==0:#3
#             cls=0
#         if  predict3==1 and predict2==1:#3
#             cls=0
        
       
        
        if cls!=3:
            if z==0:
                confusion[0][cls]=confusion[0][cls]+1
            if z==1:
                confusion[1][cls]=confusion[1][cls]+1
            if z==2:
                confusion[2][cls]=confusion[2][cls]+1
            
                
           
    # Accuracy
    acc = (confusion[0][0]+confusion[1][1]+confusion[2,2])/np.sum(confusion.flatten())*100

    # Recall
    rec1 = (confusion[0][0])/(confusion[0][0]+(confusion[0][1])+(confusion[0][2]))*100
    rec2 = (confusion[1][1])/(confusion[1][0]+(confusion[1][1])+(confusion[1][2]))*100
    rec3 = (confusion[2][2])/(confusion[2][0]+(confusion[2][1])+(confusion[2][2]))*100
    avg_rec = (rec1 + rec2 + rec3)/3

    # Precision
    prec1 = (confusion[0][0])/(confusion[0][0]+(confusion[1][0])+(confusion[2][0]))*100
    prec2 = (confusion[1][1])/(confusion[0][1]+(confusion[1][1])+(confusion[2][1]))*100
    prec3 = (confusion[2][2])/(confusion[0][2]+(confusion[1][2])+(confusion[2][2]))*100
    avg_prec = (prec1 + prec2 + prec3)/3

    #F-score
    fscore1 = 2*(prec1 * rec1)/(prec1 + rec1)
    fscore2 = 2*(prec2 * rec2)/(prec2 + rec2)
    fscore3 = 2*(prec3 * rec3)/(prec3 + rec3)
    avg_fscore = (fscore1 + fscore2 + fscore3)/3

    
    print ('Confusion Matrix \n')
    print (confusion)
    print(' ')
    print('Accuracy '+str(acc))

    print ('\n\nRecall (C1) : ', rec1)
    print ('Recall (C2) : ', rec2)
    print ('Recall (C3) : ', rec3)
    print ('Average Recall : ', avg_rec)

    print ('\n\nPrecision (C1) : ', prec1)
    print ('Precision (C2) : ', prec2)
    print ('Precision (C3) : ', prec3)
    print ('Average Precision : ', avg_prec)

    print ('\n\nF-Score (C1) : ', fscore1)
    print ('F-Score (C2) : ', fscore2)
    print ('F-Score (C3) : ', fscore3)
    print ('Average F-Score : ', avg_fscore)

# test(data,y,weights1,weights2,weights3,log=1)

data=np.concatenate((red,green,blue))
y=np.concatenate((np.full(len(red), 0),  np.full(len(green),1),np.full(len(blue),2)))
valid_data=np.concatenate((valid_red,valid_green,valid_blue))
valid_y=np.concatenate((np.full(len(valid_red), 0),  np.full(len(valid_green),1),np.full(len(valid_blue),2)))
test_data=np.concatenate((test_red,test_green,test_blue))
test_y=np.concatenate((np.full(len(test_red), 0),  np.full(len(test_green),1),np.full(len(test_blue),2)))
data_size=[len(red),len(green),len(blue)]
valid_size=[len(valid_red),len(valid_green),len(valid_blue)]
test_size=[len(test_red),len(test_green),len(test_blue)]

weights1,weights2,weights3,error=network(red,green,blue,epoch=200,log=1)

plt.plot(error)
plt.xlabel("Epochs")  # add X-axis label 
plt.ylabel("Error")  # add Y-axis label 
plt.title("Error Plot")  # add title 
plt.savefig('LS_Error.png',bbox_inches = 'tight')

generate_values_boundary(data,data_size,weights1,weights2,weights3,'Training')

test(data,y,weights1,weights2,weights3,log=1)



generate_values_boundary(valid_data,valid_size,weights1,weights2,weights3,'Validation')

test(valid_data,valid_y,weights1,weights2,weights3,log=1)



generate_values_boundary(test_data,test_size,weights1,weights2,weights3,'Test')

test(test_data,test_y,weights1,weights2,weights3,log=1)

